having to convert cnn and pipeline to pytorch from tf was like learning things abt english by learning anothrt language (eg cases, took advantage of)

need to incorporate validation and sampling of image...

LOL I thought this would be a one line switch basically but this is actually a big undertaking. 

need to read about backpropogation and gradient stuff as it relates to neural nets because i am going to be impolementing custom loss and i want to make sure i get the loss.backwards() thing right (like what if it calculates loss again>? or does it just propgate the already calculated loss through the network?)

# 
# https://machinelearningmastery.com/difference-between-backpropagation-and-stochastic-gradient-descent/
# 
# https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step
# https://www.tomasbeuzen.com/deep-learning-with-pytorch/README.html

So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient.

want to be really careful when defining the loss, because I don't want to calculate it wrong. 

what is the computational graph leading to the associated n-dimensional matrix in torch tensors? 

need to review a lot of multi/linear alg/diffy Qs this summer. 

need to fix generator for prediction without loss_weights available

definitely mention to jesse brushing up on multi/linear algebra, how I didn't need to have backward() method or make my tversky_loss function a class with inheritance (tensor has the method backward() in itself)

i don't think I need to for some reason it's forcig multi output as well...something's fishy